{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "executionInfo": {
     "elapsed": 6613,
     "status": "error",
     "timestamp": 1746779092475,
     "user": {
      "displayName": "Sabrina BENHAMOUCHE",
      "userId": "13270781467929341817"
     },
     "user_tz": -120
    },
    "id": "6NeIv8bRFvdK",
    "outputId": "88c2775f-dccf-448f-8e1f-e0e97ae891c7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/Users/sabri/Downloads/GSAF5.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1c654b5dd188>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'c:/Users/sabri/Downloads/GSAF5.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:/Users/sabri/Downloads/GSAF5.xls'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df = pd.read_excel('C:/Users/anais/Downloads/GSAF5.xls')\n",
    "print(df.head())\n",
    "\n",
    "print(df.columns)\n",
    "print(\"\\nDimensions of DataFrame :\")\n",
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "\n",
    "missing_values = df.isnull().sum() # Check for missing values in the dataset\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0])# Display columns with missing values\n",
    "\n",
    "df_copy=df.copy()\n",
    "df_copy.columns = df_copy.columns.str.strip()\n",
    "# Drop unnecessary columns\n",
    "df_copy.drop(columns=['Source', 'pdf', 'href formula', 'href', 'Case Number', 'Case Number.1', 'original order', 'Unnamed: 21', 'Unnamed: 22'],inplace=True,errors='ignore')\n",
    "\n",
    "#clean and convert Date\n",
    "def handle_invalid_dates_raw(value):\n",
    "    if isinstance(value, str):  # Check if the value is a string\n",
    "        if \"Before\" in value:  # Handle \"Before\" cases\n",
    "            return '1902-12-31'  # Placeholder date\n",
    "        elif \"-\" in value:  # Handle ranges like \"1900-1905\"\n",
    "            return value.split('-')[0] + '-01-01'  # Start of the range\n",
    "    return value  # Return as is for valid entries\n",
    "\n",
    "#Apply the cleaning function to the original data before converting\n",
    "df_copy['Date'] = df_copy['Date'].apply(handle_invalid_dates_raw)\n",
    "\n",
    "# Convert to datetime after cleaning\n",
    "df_copy['Date'] = pd.to_datetime(df_copy['Date'], errors='coerce')\n",
    "df_copy = df_copy.dropna(subset=['Date'])\n",
    "print(df_copy['Date'])\n",
    "\n",
    "#clean and convert Year\n",
    "df_copy['Extracted_Year'] = df_copy['Date'].dt.year# Extract the year from the \"Date\" column\n",
    "df_copy['Year'] = df_copy['Extracted_Year']# Replace invalid years in \"Year\" with the extracted year\n",
    "df_copy = df_copy.drop(columns=['Extracted_Year'])# Drop the temporary column\n",
    "df_copy = df_copy.dropna(subset=['Year'])# Drop Rows with NaT\n",
    "df_copy['Year'] = df_copy['Year'].astype(int)\n",
    "\n",
    "# Extract month from Date column\n",
    "df_copy[\"month\"] = df_copy[\"Date\"].dt.month\n",
    "\n",
    "#clean and convert Time\n",
    "df_copy['Time'] = df_copy['Time'].str.lower().str.strip().replace({\n",
    "    'early morning': '06:00', 'morning': '09:00', 'midday': '12:00',\n",
    "    'afternoon': '15:00', 'late afternoon': '16:30', 'evening': '19:00',\n",
    "    'night': '21:00', 'midnight': '00:00', 'dawn': '05:00', 'sunset': '18:30',\n",
    "    'lunchtime': '12:30', 'just before noon': '11:50'\n",
    "\n",
    "}) # Replaces descriptive time values with standard HH:MM format.\n",
    "\n",
    "df_copy['Time'] = df_copy['Time'].str.extract(r'(\\d{1,2}[h:]?\\d{0,2})') # Captures numeric time formats like \"14h30\", \"14:30\", or \"1430\".\n",
    "df_copy['Time'] = df_copy['Time'].str.replace('h', ':').str.replace('\"', '').str.strip()# Standardize separators ('h' → ':'), remove quotes, and strip spaces\n",
    "df_copy['Time'] = df_copy['Time'].str.replace(r'(\\d{2})(\\d{2})', r'\\1:\\2', regex=True) # Inserts a colon (:) between hours and minutes, e.g., \"1430\" → \"14:30\"\n",
    "\n",
    "#How Regex Works Here\n",
    "#Pattern Explanation (r'(\\d{1,2}[h:]?\\d{0,2})'):\n",
    "#- \\d{1,2} → Captures 1 or 2 digits (hours, like \"14\" in \"14h30\")\n",
    "#- [h:]? → Matches an optional \"h\" or \":\" separator\n",
    "#- \\d{0,2} → Captures up to 2 more digits (minutes, like \"30\" in \"14h30\")\n",
    "\n",
    "#Manipulating String:\n",
    "# Strip whitespace, convert to lowercase, and remove special characters in text columns\n",
    "text_column=['Country','State','Location','Activity','Name','Sex','Injury','Species']\n",
    "for col in text_column:\n",
    "    df_copy[col] = df_copy[col].str.lower().str.strip().str.replace(r'[^a-z0-9\\s-]', '', regex=True)#removing all non-alphabetic characters, allow numbers to remain\n",
    "\n",
    "# Clean activity text\n",
    "df_copy['Activity'] = df_copy['Activity'].str.strip().str.lower()\n",
    "\n",
    "# Define categorization function\n",
    "def categorize_activity(activity):\n",
    "    if pd.isna(activity):\n",
    "        return 'unknown'\n",
    "    activity = activity.lower()\n",
    "    if 'surf' in activity:\n",
    "        return 'surfing'\n",
    "    elif 'swim' in activity:\n",
    "        return 'swimming'\n",
    "    elif 'fish' in activity:\n",
    "        return 'fishing'\n",
    "    elif 'snorkel' in activity:\n",
    "        return 'snorkeling'\n",
    "    elif 'scuba' in activity:\n",
    "        return 'scuba diving'\n",
    "    elif 'dive' in activity:\n",
    "        return 'diving'\n",
    "    elif 'wading' in activity or 'wade' in activity:\n",
    "        return 'wading'\n",
    "    elif 'stand' in activity or 'treading' in activity:\n",
    "        return 'standing/treading'\n",
    "    elif 'kayak' in activity:\n",
    "        return 'kayaking'\n",
    "    elif 'board' in activity:\n",
    "        return 'boarding'\n",
    "    elif 'bathing' in activity:\n",
    "        return 'bathing'\n",
    "    elif 'fell' in activity or 'overboard' in activity:\n",
    "        return 'fell overboard'\n",
    "    elif 'walk' in activity:\n",
    "        return 'walking'\n",
    "    elif 'paddle' in activity:\n",
    "        return 'paddling'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Apply categorization\n",
    "df_copy['Activity'] = df_copy['Activity'].apply(categorize_activity)\n",
    "# Display the result\n",
    "print(df_copy['Activity'].head(10))\n",
    "\n",
    "# Clean Fatal Y/N text\n",
    "df_copy['Fatal Y/N'] = df_copy['Fatal Y/N'].astype(str).str.lower().str.strip().replace({\n",
    "'n': 'No', 'f': 'No', 'm': 'No', 'unknown': 'Unknown', 'nq': 'Unknown','y':'Yes','2017':'Unknown'\n",
    "})\n",
    "\n",
    "# Clean Sex text\n",
    "df_copy['Sex'] = df_copy['Sex'].replace({'N': 'Unknown', 'lli': 'Unknown'})# Replace incorrect values with 'Unknown'\n",
    "\n",
    "# Dictionary for standardizing country names\n",
    "country_mapping = {\n",
    "    'maldives': 'Maldives',\n",
    "    'maldive islands': 'Maldives',\n",
    "    'turks and caicos': 'Turks & Caicos',\n",
    "    'turks  caicos': 'Turks & Caicos',\n",
    "    'reunion island': 'Reunion',\n",
    "    'united kingdom': 'UK',\n",
    "    'england': 'UK',\n",
    "    'hawaii': 'USA',\n",
    "    'columbia': 'Colombia',\n",
    "    'cali': 'California',\n",
    "    'wa': 'Washington',\n",
    "    'az': 'Arizona',\n",
    "    'south korea': 'Korea',\n",
    "    'korea': 'Korea',\n",
    "    'ceylon sri lanka': 'Sri Lanka'\n",
    "}\n",
    "\n",
    "# Apply standardization\n",
    "df_copy['Country'] = df_copy['Country'].replace(country_mapping).str.strip().str.title()\n",
    "\n",
    "# Valid States\n",
    "us_states = [\n",
    "    \"Florida\", \"California\", \"Hawaii\", \"Texas\", \"New York\",\n",
    "    \"North Carolina\", \"South Carolina\", \"Georgia\", \"New Jersey\",\n",
    "    \"Washington\", \"Virginia\", \"Alabama\", \"Oregon\", \"Maine\", \"Massachusetts\",\n",
    "    \"Maryland\", \"Louisiana\", \"Delaware\", \"Rhode Island\", \"Puerto Rico\", \"New Mexico\", \"Utah\"\n",
    "]\n",
    "df_copy['State'] = df_copy['State'].str.strip().str.title()\n",
    "\n",
    "#Filter to keep only the US states\n",
    "df_Cleaned = df_copy[df_copy['State'].isin(us_states)]\n",
    "\n",
    "# Check after cleaning\n",
    "print(sorted(df_Cleaned['State'].unique()))\n",
    "\n",
    "#Clean Location text\n",
    "# Define classification keywords\n",
    "coastal_keywords =  coastal_keywords = [\n",
    "    'beach', 'shore', 'coast', 'reef', 'bay', 'harbor', 'lagoon', 'cove',\n",
    "    'tidal', 'estuary', 'marsh', 'dune', 'bluff', 'cliff', 'seaside',\n",
    "    'waterfront', 'inlet', 'jetty', 'pier', 'promenade', 'boardwalk',\n",
    "    'tideline', 'headland', 'breakwater', 'tidal flat', 'mangrove',\n",
    "    'foreshore', 'backshore', 'coastline', 'sandy shore', 'rocky shore',\n",
    "    'bayou', 'delta', 'nearshore', 'littoral', 'bayside', 'surf zone',\n",
    "    'tidal pool', 'fishing village', 'marina', 'dock', 'berth'\n",
    "]\n",
    "open_water_keywords = open_water_keywords = [\n",
    "    'sea', 'ocean', 'offshore', 'deep', 'open water',\n",
    "    'pelagic', 'bluewater', 'high seas', 'abyss', 'deep sea',\n",
    "    'mid-ocean', 'submarine plain', 'open ocean', 'marine',\n",
    "    'international waters', 'deep ocean', 'vast sea', 'beyond reef',\n",
    "    'deep blue', 'open expanse', 'deep current', 'underwater plain',\n",
    "    'off the coast', 'mid-sea', 'trench', 'basin', 'drift zone',\n",
    "    'continental slope', 'continental rise'\n",
    "]\n",
    "\n",
    "# Classification function\n",
    "def classify_location(location):\n",
    "    location = str(location).lower().strip()  # Normalize text\n",
    "    if any(keyword in location for keyword in coastal_keywords):\n",
    "        return \"Coastal/Reef\"\n",
    "    elif any(keyword in location for keyword in open_water_keywords):\n",
    "        return \"Open Waters\"\n",
    "    return \"Unknown\"  # If no keyword matches\n",
    "\n",
    "# Apply classification\n",
    "df_copy['Location'] = df_copy['Location'].apply(classify_location)\n",
    "\n",
    "# CLEANING INJURY COLUMN:\n",
    "# Define categorization function\n",
    "def categorize_injury(injury):\n",
    "    if pd.isna(injury):\n",
    "        return 'unknown'\n",
    "    injury = injury.lower()\n",
    "    if 'bite' in injury:\n",
    "        return 'bite'\n",
    "    elif 'attack' in injury:\n",
    "        return 'attack'\n",
    "    elif 'injury' in injury:\n",
    "        return 'injury'\n",
    "    elif 'fatal' in injury:\n",
    "        return 'fatal'\n",
    "    elif 'non-fatal' in injury:\n",
    "        return 'non-fatal'\n",
    "    else:\n",
    "        return 'other'\n",
    "# Apply categorization\n",
    "df_copy['Injury'] = df_copy['Injury'].apply(categorize_injury)\n",
    "\n",
    "#CLEANING SPECIES COLUMN:\n",
    "# Define categorization function\n",
    "def categorize_species(species):\n",
    "    if pd.isna(species):\n",
    "        return 'unknown'\n",
    "    species = species.lower()\n",
    "    if 'great white' in species or 'white shark' in species:\n",
    "        return 'great white shark'\n",
    "    elif 'tiger' in species:\n",
    "        return 'tiger shark'\n",
    "    elif 'bull' in species:\n",
    "        return 'bull shark'\n",
    "    elif 'hammerhead' in species:\n",
    "        return 'hammerhead shark'\n",
    "    elif 'whale' in species:\n",
    "        return 'whale shark'\n",
    "    elif 'mako' in species:\n",
    "        return 'mako shark'\n",
    "    elif 'lemon' in species:\n",
    "        return 'lemon shark'\n",
    "    elif 'nurse' in species:\n",
    "        return 'nurse shark'\n",
    "    else:\n",
    "        return 'other'\n",
    "# Apply categorization\n",
    "df_copy['Species'] = df_copy['Species'].apply(categorize_species)\n",
    "\n",
    "#Drop missing values\n",
    "\n",
    "variables=['State','Location','Activity','Sex','Fatal Y/N','Name','Species','Time']\n",
    "df_copy.dropna(subset=['State','Location','Activity','Sex','Fatal Y/N','Name','Species','Time'], inplace=True)\n",
    "\n",
    "\n",
    "#Fill missing value\n",
    "var_=['Year','Type','Country','Injury']\n",
    "for col in var_:\n",
    "    df_copy[col] = df_copy[col].fillna(\"Unknown\")\n",
    "\n",
    "df_copy['Age'] = pd.to_numeric(df_copy['Age'], errors='coerce')  # Converts invalid ages to NaN\n",
    "df_copy['Age'] = df_copy['Age'].fillna(df_copy['Age'].median())\n",
    "\n",
    "\n",
    "# Verify changes\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(df_copy.isnull().sum())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc_d4aysIRz6"
   },
   "source": [
    "Check hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9FithFpIVaP"
   },
   "outputs": [],
   "source": [
    "# Check the distribution for location\n",
    "print(df_copy['Location'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkweagPJIYua"
   },
   "source": [
    "Initial Insights:\n",
    "\n",
    "High concentration of attacks near reefs and coastal areas, supporting the hypothesis that sharks are more active where humans interact with shallow waters.\n",
    "\n",
    "Only a small fraction in open waters, possibly because humans rarely venture far into deep ocean zones where shark encounters are less frequent.\n",
    "\n",
    "A large number of \"Unknown\" locations, which could benefit from better classification—maybe using an API for geolocation or refining text recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5D-SU4RIb1x"
   },
   "outputs": [],
   "source": [
    "# Filter out \"Unknown\" locations for clearer visualization\n",
    "df_filtered = df_copy[df_copy['Location'] != \"Unknown\"]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=df_filtered, x='Location', color='coral')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Shark Attacks Near Coasts vs Open Waters\")\n",
    "plt.xlabel(\"Location Type\")\n",
    "plt.ylabel(\"Number of Incidents\")\n",
    "#plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Count occurrences\n",
    "location_counts = df_copy['Location'].value_counts()\n",
    "\n",
    "# Create the pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(location_counts, labels=location_counts.index, autopct='%1.1f%%', colors=['lightblue', 'coral', 'grey'])\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Distribution of Shark Attacks by Location Type\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Filter only Coastal/Reef incidents\n",
    "coastal_attacks = df_copy[df_copy[\"Location\"] == \"Coastal/Reef\"]\n",
    "\n",
    "# Group by year and count attacks\n",
    "trend_data = coastal_attacks.groupby(\"Year\")[\"Location\"].count().reset_index()\n",
    "\n",
    "# Display rows\n",
    "print(trend_data)\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=trend_data, x=\"Year\", y=\"Location\", marker=\"o\", color=\"coral\")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Trend of Shark Attacks Near Reefs & Coastlines Over Time\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Attacks\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Count attacks per country\n",
    "country_counts = df_copy['Country'].value_counts().reset_index()\n",
    "country_counts.columns = ['Country', 'Attack_Count']\n",
    "\n",
    "# Display top 10 most affected countries\n",
    "print(country_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkrxO8l_IvTi"
   },
   "source": [
    "This confirms that USA, Australia, and South Africa experience the highest shark attack rates, aligning with our hypothesis. Here's a breakdown of possible contributing factors:\n",
    "\n",
    "USA (1,526 attacks) → Large coastal population, high tourism, many surfers/swimmers. Florida alone accounts for a significant share of attacks.\n",
    "\n",
    "Australia (657 attacks) → Extensive coastline, thriving marine ecosystem with apex predators like great white sharks, and strong surfing culture.\n",
    "\n",
    "South Africa (338 attacks) → Known for shark hotspots like Gansbaai and False Bay, with seasonal movements of great whites near shorelines.\n",
    "\n",
    "Other countries (New Zealand, Bahamas, Brazil, etc.) → Lower attack counts, likely due to fewer human interactions with sharks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IukK6F3Iw8z"
   },
   "outputs": [],
   "source": [
    "# Select the top 10 countries\n",
    "top_countries = country_counts.head(10)\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_countries[\"Country\"], y=top_countries[\"Attack_Count\"], palette=\"coolwarm\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Shark Attack Frequency by Country (Top 10)\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Number of Incidents\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Count shark attacks by state\n",
    "state_counts = df_Cleaned['State'].value_counts()\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=state_counts.index, y=state_counts.values, palette='Blues')\n",
    "\n",
    "# Add labels and adjust rotation for better readability\n",
    "plt.title(\"Number of Shark Attacks by State in the USA\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Number of Attacks\")\n",
    "plt.xticks(rotation=45)  # Rotate labels to avoid overlap\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Count attacks per activity\n",
    "activity_counts = df_copy[\"Activity\"].value_counts().reset_index()\n",
    "activity_counts.columns = [\"Activity\", \"Attack_Count\"]\n",
    "\n",
    "# Display the top activities involved in shark attacks\n",
    "print(activity_counts)\n",
    "\n",
    "# Calculate percentage of total attacks per activity\n",
    "activity_counts[\"Percentage\"] = (activity_counts[\"Attack_Count\"] / activity_counts[\"Attack_Count\"].sum()) * 100\n",
    "\n",
    "# Display top 10 risky activities\n",
    "print(activity_counts.head(10))\n",
    "\n",
    "# Select top 5 risky activities\n",
    "#top_activities = activity_counts\n",
    "top_activities = activity_counts.head(5)\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(top_activities[\"Attack_Count\"], labels=top_activities[\"Activity\"], autopct=\"%1.1f%%\", colors=[\"lightblue\", \"coral\", \"grey\", \"gold\", \"green\"])\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Shark Attack Distribution by Activity\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BP_sOWZJBCr"
   },
   "source": [
    "Surfing (42.3%) → The most at-risk activity, likely due to surfers spending extended time in the water and making movements that resemble prey.\n",
    "\n",
    "Swimming (24.1%) → Also a high-risk activity, possibly because swimmers are often in shallow waters where sharks hunt.\n",
    "\n",
    "Fishing (15.8%) → May attract sharks due to bait and fish activity near boats.\n",
    "\n",
    "Other Activities (12%) → Includes various water-based activities that don’t fit major categories.\n",
    "\n",
    "Boarding (5.8%) → A lower-risk activity compared to the others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnNSdotvJB6s"
   },
   "outputs": [],
   "source": [
    "# Group by month\n",
    "df_copy[\"month\"] = df_copy[\"Date\"].dt.month\n",
    "\n",
    "month_counts = df_copy.groupby(\"month\")[\"Injury\"].count().reset_index()\n",
    "\n",
    "# Display results\n",
    "print(month_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDGonFE4JHlN"
   },
   "source": [
    "January (1,903 incidents) shows an extreme spike—this seems highly unusual and could indicate data inconsistencies or an issue in data recording.\n",
    "\n",
    "Relatively stable attack rates across other months, with no clear seasonal peak—June to September do not show a significant rise compared to colder months.\n",
    "\n",
    "September (112 attacks) has a slightly higher count, which could be worth exploring further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AL8w4PyJIdu"
   },
   "outputs": [],
   "source": [
    "# Create time-series plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(data=month_counts, x=\"month\", y=\"Injury\", marker=\"o\", color=\"coral\")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Seasonal Trend of Shark attaks by Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Injuries\")\n",
    "plt.xticks(range(1, 13), [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "df_copy['Fatal Y/N'].unique()\n",
    "df_copy[\"Fatal Y/N\"] = df_copy[\"Fatal Y/N\"].replace([\"nan\", \"NaN\"], \"Unknown\")\n",
    "# Count occurrences of Fatal and Non-Fatal cases\n",
    "fatal_counts = df_copy[\"Fatal Y/N\"].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(fatal_counts)\n",
    "# Define colors for clarity\n",
    "colors = {\"No\": \"green\", \"Yes\": \"red\", \"Unknown\": \"gray\"}\n",
    "\n",
    "# Create the bar plot\n",
    "df_copy[\"Fatal Y/N\"].value_counts().plot(kind=\"bar\", color=[colors[val] for val in df_copy[\"Fatal Y/N\"].unique()])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Fatal Outcome\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Fatal and Non-Fatal Shark Attacks\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis=\"y\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s_UpP8EJQ27"
   },
   "source": [
    "Most shark attack cases are non-fatal → 2449 incidents resulted in no fatality, making up the majority of recorded cases. This suggests that while shark attacks do occur, they are rarely deadly.\n",
    "\n",
    "Fatal shark attacks account for a smaller proportion → 474 incidents led to fatalities. This indicates that although serious injuries can happen, most victims survive shark encounters.\n",
    "\n",
    "120 cases classified as \"Unknown\" → These could be due to missing data, inconsistent reporting, or unclear outcomes in historical records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4vRjLRiJRuf"
   },
   "source": [
    "CONCLUSION\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hypothesis 1 (Coastal/Reef vs Open Waters) → Supported. Most attacks occur near coastlines and reefs, likely due to higher human activity in these areas. Open water incidents remain rare.\n",
    "\n",
    "Hypothesis 2 (Regional Differences in Attack Rates) → Confirmed. USA, Australia, and South Africa are the top three high-risk regions, likely due to shark population, coastal tourism, and water activities.\n",
    "\n",
    "Hypothesis 3 (Activity Influence on Shark Encounters) → Validated. Surfing and swimming account for most shark attacks, confirming the risk factor linked to extended water exposure and movement patterns resembling prey.\n",
    "\n",
    "Hypothesis 4 (Seasonality & Shark Attacks) → Partially Supported. While some months show fluctuations, the expected summer peak isn’t as strong as anticipated. January’s spike is unusual, suggesting potential data inconsistencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1kYl6B9JUo5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "15KQHgio9RZR1Qp2H8Xe7jUiKfkmvXPt2",
     "timestamp": 1746779292074
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
